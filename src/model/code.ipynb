{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar librerias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtener el conjunto de datos\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Diet\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if there are any null values\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Seleccionar las características relevantes para el SOM (Age, Cholesterol, Blood Pressure, ...)\n",
    "features = [\n",
    "    # \"Patient ID\",\n",
    "    \"Age\",\n",
    "    \"Sex\",\n",
    "    \"Blood Pressure\",\n",
    "    \"Smoking\",\n",
    "    \"Cholesterol\",\n",
    "    \"Heart Rate\",\n",
    "    \"Diabetes\",\n",
    "    \"Family History\",\n",
    "    \"Alcohol Consumption\",\n",
    "    \"Exercise Hours Per Week\",\n",
    "    \"Diet\",\n",
    "    \"Previous Heart Problems\",\n",
    "    \"Sedentary Hours Per Day\",\n",
    "    \"BMI\",\n",
    "    \"Physical Activity Days Per Week\",\n",
    "    \"Sleep Hours Per Day\",\n",
    "    \"Heart Attack Risk\",\n",
    "    \"Country\",\n",
    "    \"Continent\",\n",
    "    \"Hemisphere\",\n",
    "]\n",
    "df = df[features]\n",
    "\n",
    "# Dividir la columna 'Blood Pressure' en dos columnas separadas\n",
    "df[[\"Systolic Pressure\", \"Diastolic Pressure\"]] = df[\"Blood Pressure\"].str.split(\n",
    "    \"/\", expand=True\n",
    ")\n",
    "df.drop(columns=[\"Blood Pressure\"], inplace=True)\n",
    "df[\"Systolic Pressure\"] = df[\"Systolic Pressure\"].astype(\"int64\")\n",
    "df[\"Diastolic Pressure\"] = df[\"Diastolic Pressure\"].astype(\"int64\")\n",
    "\n",
    "# Convertir las columnas a valores numéricos\n",
    "le = LabelEncoder()\n",
    "df[\"Sex\"] = le.fit_transform(df[\"Sex\"])\n",
    "region = df.pop(\"Sex\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"Diet\"] = le.fit_transform(df[\"Diet\"])\n",
    "region = df.pop(\"Diet\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"Country\"] = le.fit_transform(df[\"Country\"])\n",
    "region = df.pop(\"Country\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"Continent\"] = le.fit_transform(df[\"Continent\"])\n",
    "region = df.pop(\"Continent\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"Hemisphere\"] = le.fit_transform(df[\"Hemisphere\"])\n",
    "region = df.pop(\"Hemisphere\")\n",
    "\n",
    "df[\"Chol_BMI_ratio\"] = df.apply(lambda r: float(r[\"Cholesterol\"] / r[\"BMI\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph of the corr\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(\"Heart Attack Risk\", axis=1),\n",
    "    df[\"Heart Attack Risk\"],\n",
    "    random_state=42,\n",
    "    test_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos el Id del pasajero en el conjunto de datos de prueba\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)\n",
    "predictions = perceptron.predict(X_test)\n",
    "# Calculate his accuracy of the perceptron\n",
    "acc_perceptron = accuracy_score(y_test, predictions)\n",
    "# Print the accuracy\n",
    "print(acc_perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "# print accuracy\n",
    "acc_gnb =accuracy_score(y_test, y_pred)\n",
    "print(acc_gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Regresion Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model_SGD = SGDClassifier(loss='log_loss',learning_rate='constant',eta0=0.1 ) # investicar los parámetros en la documentacion y variar el learning_rate\n",
    "logistic_model_SGD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Clases de la variable dependiente: {logistic_model_SGD.classes_}')\n",
    "print('\\n')\n",
    "print('Vectores de coeficientes:')\n",
    "print(logistic_model_SGD.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic_model_SGD.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy entrenamiento: {accuracy_score(y_train, y_pred)}')\n",
    "print('Matriz de confusión:')\n",
    "matriz_confusion = confusion_matrix(y_train, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion, display_labels=logistic_model_SGD.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = logistic_model_SGD.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy testing: {accuracy_score(y_test, y_pred_test)}')\n",
    "print('Matriz de confusión:')\n",
    "matriz_confusion_test = confusion_matrix(y_test, y_pred_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_test, display_labels=logistic_model_SGD.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Clases de la variable dependiente: {logistic_model.classes_}')\n",
    "print('\\n')\n",
    "print('Vectores de coeficientes:')\n",
    "print(logistic_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = logistic_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy entrenamiento: {accuracy_score(y_train, y_pred_2)}')\n",
    "print('Matriz de confusión:')\n",
    "matriz_confusion_2 = confusion_matrix(y_train, y_pred_2)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_2, display_labels=logistic_model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_2 = logistic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy testing: {accuracy_score(y_test, y_pred_test_2)}')\n",
    "print('Matriz de confusión:')\n",
    "matriz_confusion_test_2 = confusion_matrix(y_test, y_pred_test_2)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_test_2, display_labels=logistic_model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use network neural\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Con sequetial podemos construir una red neuronal apilando capas\n",
    "modelsequ = Sequential()\n",
    "modelsequ.add(Input(shape=(X_train.shape[1],)))\n",
    "modelsequ.add(Dense(15, activation='relu'))\n",
    "modelsequ.add(Dense(10, activation='relu'))\n",
    "modelsequ.add(Dense(1, activation='sigmoid'))\n",
    "modelsequ.compile(loss='binary_crossentropy',  optimizer='adam', metrics=['accuracy'])\n",
    "modelsequ.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyseq = modelsequ.fit(\n",
    "    X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph of loss \n",
    "plt.plot(historyseq.history['loss'])\n",
    "plt.plot(historyseq.history['val_loss'])\n",
    "plt.title('Loss vs Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lib for convolutional neural network\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelconv = Sequential()\n",
    "# modelconv.add(tf.keras.Input(shape=(10,)))\n",
    "modelconv.add(Conv1D(32, 3, activation=\"relu\", input_shape=(X_train.shape[1], 1)))\n",
    "modelconv.add(MaxPooling1D())\n",
    "modelconv.add(Conv1D(64, 3, activation=\"relu\"))\n",
    "modelconv.add(MaxPooling1D())\n",
    "modelconv.add(Flatten())\n",
    "modelconv.add(Dense(64, activation=\"relu\"))\n",
    "modelconv.add(Dropout(0.5))\n",
    "modelconv.add(Dense(1, activation=\"sigmoid\"))\n",
    "modelconv.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "modelconv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "historyconv = modelconv.fit(\n",
    "    X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=16\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
