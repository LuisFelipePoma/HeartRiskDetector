{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar librerias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtener el conjunto de datos\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Diet\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if there are any null values\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Seleccionar las características relevantes para el SOM (Age, Cholesterol, Blood Pressure, ...)\n",
    "features = [\n",
    "    # \"Patient ID\",\n",
    "    \"Age\",\n",
    "    \"Sex\",\n",
    "    \"Blood Pressure\",\n",
    "    \"Smoking\",\n",
    "    # \"Cholesterol\",\n",
    "    \"Heart Rate\",\n",
    "    # \"Diabetes\",\n",
    "    # \"Family History\",\n",
    "    \"Alcohol Consumption\",\n",
    "    \"Exercise Hours Per Week\",\n",
    "    \"Diet\",\n",
    "    # \"Previous Heart Problems\",\n",
    "    # \"Sedentary Hours Per Day\",\n",
    "    \"BMI\",\n",
    "    # \"Physical Activity Days Per Week\",\n",
    "    # \"Sleep Hours Per Day\",\n",
    "    \"Heart Attack Risk\",\n",
    "]\n",
    "df = df[features]\n",
    "\n",
    "# Dividir la columna 'Blood Pressure' en dos columnas separadas\n",
    "df[[\"Systolic Pressure\", \"Diastolic Pressure\"]] = df[\"Blood Pressure\"].str.split(\n",
    "    \"/\", expand=True\n",
    ")\n",
    "df[\"Systolic Pressure\"] = pd.to_numeric(df[\"Systolic Pressure\"])\n",
    "df[\"Diastolic Pressure\"] = pd.to_numeric(df[\"Diastolic Pressure\"])\n",
    "df.drop(columns=[\"Blood Pressure\"], inplace=True)\n",
    "\n",
    "# Convertir la columna \"Diet\" (['Average', 'Unhealthy', 'Healthy']) a numerico\n",
    "df[\"Diet\"] = df[\"Diet\"].apply(\n",
    "    lambda x: 0 if x == \"Unhealthy\" else 1 if x == \"Average\" else 2\n",
    ")\n",
    "\n",
    "# Convertir las columnas a valores numéricos\n",
    "# df[\"Patient ID\"] = df[\"Patient ID\"].apply(lambda x: x[3:])\n",
    "\n",
    "# Convertir las columnas a valores numéricos\n",
    "df[\"Sex\"] = df[\"Sex\"].apply(lambda x: 1 if x == \"Male\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph of the corr\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(\"Heart Attack Risk\", axis=1),\n",
    "    df[\"Heart Attack Risk\"],\n",
    "    random_state=0,\n",
    "    test_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos el Id del pasajero en el conjunto de datos de prueba\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)\n",
    "predictions = perceptron.predict(X_test)\n",
    "# Calculate his accuracy of the perceptron\n",
    "acc_perceptron = accuracy_score(y_test, predictions)\n",
    "# Print the accuracy\n",
    "print(acc_perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "# print accuracy\n",
    "acc_gnb =accuracy_score(y_test, y_pred)\n",
    "print(acc_gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Regresion Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model_SGD = SGDClassifier(loss='log_loss',learning_rate='constant',eta0=0.1 ) # investicar los parámetros en la documentacion y variar el learning_rate\n",
    "logistic_model_SGD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Clases de la variable dependiente: {logistic_model_SGD.classes_}')\n",
    "print('\\n')\n",
    "print('Vectores de coeficientes:')\n",
    "print(logistic_model_SGD.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic_model_SGD.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy entrenamiento: {accuracy_score(y_train, y_pred)}')\n",
    "print('Matriz de confusión:')\n",
    "matriz_confusion = confusion_matrix(y_train, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion, display_labels=logistic_model_SGD.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = logistic_model_SGD.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy testing: {accuracy_score(y_test, y_pred_test)}')\n",
    "print('Matriz de confusión:')\n",
    "matriz_confusion_test = confusion_matrix(y_test, y_pred_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_test, display_labels=logistic_model_SGD.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Clases de la variable dependiente: {logistic_model.classes_}')\n",
    "print('\\n')\n",
    "print('Vectores de coeficientes:')\n",
    "print(logistic_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = logistic_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy entrenamiento: {accuracy_score(y_train, y_pred_2)}')\n",
    "print('Matriz de confusión:')\n",
    "matriz_confusion_2 = confusion_matrix(y_train, y_pred_2)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_2, display_labels=logistic_model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_2 = logistic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy testing: {accuracy_score(y_test, y_pred_test_2)}')\n",
    "print('Matriz de confusión:')\n",
    "matriz_confusion_test_2 = confusion_matrix(y_test, y_pred_test_2)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_test_2, display_labels=logistic_model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use network neural\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Con sequetial podemos construir una red neuronal apilando capas\n",
    "modelsequ = Sequential()\n",
    "modelsequ.add(tf.keras.Input(shape=(10,), dtype=tf.string))\n",
    "modelsequ.add(Dense(10, activation='relu'))\n",
    "modelsequ.add(Dense(1, activation='sigmoid'))\n",
    "modelsequ.compile(loss='binary_crossentropy',  optimizer='adam', metrics=['accuracy'])\n",
    "modelsequ.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyseq = modelsequ.fit(\n",
    "    X_train, y_train, validation_data=(X_test, y_test), epochs=500\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
